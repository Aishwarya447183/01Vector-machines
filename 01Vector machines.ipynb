{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb8762-7c46-4463-87a1-0241db6e5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q1.\n",
    "\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) can be represented as follows:\n",
    "\n",
    "Given a training dataset with input vectors xᵢ and corresponding binary class labels yᵢ, where i ranges from 1 to the number of training samples, the linear SVM aims to find a hyperplane that separates the data points of different classes with the maximum margin.\n",
    "\n",
    "The decision function for a linear SVM can be written as:\n",
    "\n",
    "f(x) = w·x + b\n",
    "\n",
    "where:\n",
    "\n",
    "f(x) is the decision function that predicts the class label for input vector x,\n",
    "w is a vector of weights (coefficients) that determines the orientation of the hyperplane,\n",
    "· denotes the dot product between vectors,\n",
    "b is the bias term (a scalar) that determines the offset of the hyperplane from the origin.\n",
    "The goal of training a linear SVM is to find the optimal values for the weight vector w and the bias term b, such that the margin between the hyperplane and the closest data points (known as support vectors) of each class is maximized while maintaining correct classification.\n",
    "\n",
    "The optimization problem can be formulated as:\n",
    "\n",
    "minimize: ½||w||² + C∑ξᵢ\n",
    "\n",
    "subject to: yᵢ(w·xᵢ + b) ≥ 1 - ξᵢ, for all training samples (i)\n",
    "\n",
    "where:\n",
    "\n",
    "||w||² represents the squared Euclidean norm of the weight vector w,\n",
    "C is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the classification error,\n",
    "ξᵢ are slack variables that allow for some training samples to be misclassified or fall within the margin.\n",
    "By solving this optimization problem, the linear SVM finds the optimal hyperplane that maximizes the margin between classes while allowing for a certain degree of misclassification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40077dd-6358-4145-975b-597b4d49ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.\n",
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is derived from the optimization problem that aims to find the optimal hyperplane separating the data points of different classes with the maximum margin. The objective function consists of two components: the margin maximization term and the regularization term.\n",
    "\n",
    "The objective function for a linear SVM can be written as:\n",
    "\n",
    "minimize: ½||w||² + C∑ξᵢ\n",
    "\n",
    "where:\n",
    "\n",
    "½||w||² is the margin maximization term, which represents half the squared Euclidean norm of the weight vector w. This term encourages the SVM to find a hyperplane with a large margin between classes.\n",
    "C∑ξᵢ is the regularization term, which penalizes the misclassification or the data points that fall within the margin. C is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the classification error. ξᵢ represents the slack variables associated with the training samples and measures the extent to which a sample violates the margin or is misclassified. The sum (∑) is taken over all training samples.\n",
    "The objective function aims to minimize the sum of these two terms. The margin maximization term ensures that the hyperplane is well-separated from the support vectors, while the regularization term balances the desire for a larger margin with the need to avoid overfitting by allowing for some misclassification.\n",
    "\n",
    "By solving the optimization problem and minimizing the objective function, the linear SVM finds the optimal values for the weight vector w and the bias term b, resulting in a hyperplane that maximizes the margin between classes while controlling the classification error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2bcdd-3c05-4541-b781-c7c905a05ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q3.\n",
    "\n",
    "The kernel trick is a technique used in Support Vector Machines (SVMs) to implicitly map data points into higher-dimensional feature spaces without actually computing the transformed feature vectors explicitly. It allows SVMs to efficiently handle non-linearly separable data by using kernel functions.\n",
    "\n",
    "In a standard SVM, the decision boundary is a linear hyperplane in the input space. However, many real-world datasets are not linearly separable in their original form. The kernel trick enables SVMs to find non-linear decision boundaries by implicitly mapping the input data to a higher-dimensional space where linear separation becomes possible.\n",
    "\n",
    "The kernel trick works by defining a kernel function, typically denoted as K(x, y), which computes the inner product between the feature vectors of two data points x and y in the higher-dimensional space. This kernel function represents a similarity measure between the input vectors without explicitly transforming them. By using the kernel function, the SVM can operate in the original input space while implicitly utilizing the higher-dimensional feature space.\n",
    "\n",
    "The kernel function allows the SVM to take advantage of the kernel trick by computing the decision function as:\n",
    "\n",
    "f(x) = Σ αᵢ yᵢ K(x, xᵢ) + b\n",
    "\n",
    "where:\n",
    "\n",
    "f(x) is the decision function that predicts the class label for input vector x,\n",
    "αᵢ and yᵢ are the Lagrange multipliers and class labels, respectively, obtained during the SVM training process,\n",
    "K(x, xᵢ) is the kernel function that computes the inner product (similarity) between the input vector x and a support vector xᵢ in the higher-dimensional space,\n",
    "b is the bias term.\n",
    "Commonly used kernel functions include:\n",
    "\n",
    "Linear Kernel: K(x, y) = x·y, which represents the inner product of the input vectors in the original space.\n",
    "Polynomial Kernel: K(x, y) = (αx·y + c)ᵈ, which computes the polynomial similarity between the input vectors with parameters α, c, and d.\n",
    "Gaussian (RBF) Kernel: K(x, y) = exp(-γ||x - y||²), which measures the similarity based on the Gaussian distribution with parameter γ.\n",
    "By choosing an appropriate kernel function, the SVM can effectively capture complex non-linear relationships in the data, making it a powerful tool for classification tasks. The kernel trick allows SVMs to leverage the benefits of non-linear mapping while avoiding the computational cost associated with explicit feature space transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ecd892-d4b5-40bb-8002-a3eee936bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q4.\n",
    "In Support Vector Machines (SVM), support vectors play a crucial role in determining the decision boundary and maximizing the margin between classes. Support vectors are the subset of training data points that lie closest to the decision boundary or are misclassified.\n",
    "\n",
    "Let's consider a simple example with a two-class classification problem. Suppose we have two classes, class A and class B, and the goal is to find a decision boundary that separates the two classes.\n",
    "\n",
    "In a linear SVM, the decision boundary is represented by a hyperplane. The support vectors are the data points from both classes that are closest to the decision boundary. These points influence the position and orientation of the decision boundary.\n",
    "\n",
    "The key characteristics of support vectors in SVM are as follows:\n",
    "\n",
    "Support vectors define the decision boundary: The decision boundary is determined by the support vectors. The SVM aims to find the hyperplane that maximizes the margin, which is the distance between the decision boundary and the support vectors. This margin is crucial for good generalization and robustness of the SVM classifier.\n",
    "\n",
    "Support vectors are influential points: Support vectors have a significant impact on the SVM model. They define the margin and are responsible for the classification decision. Removing or modifying support vectors can lead to changes in the decision boundary and classification outcomes.\n",
    "\n",
    "Support vectors handle non-linear separations: In non-linear SVMs, where kernel functions are used, support vectors are critical for capturing complex decision boundaries. The kernel trick allows the SVM to implicitly map the data to a higher-dimensional feature space, where a linear separation is possible. The support vectors in the transformed space help define the non-linear decision boundary.\n",
    "\n",
    "In summary, support vectors in SVM are the training data points that have the most influence on determining the decision boundary. They lie closest to the decision boundary or are misclassified examples. By using support vectors, SVM maximizes the margin between classes and achieves effective classification, even in cases of non-linear separability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4680da-c152-42eb-b06c-e4c01fc63ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q6.\n",
    "\n",
    "Certainly! Here's an implementation of a linear SVM classifier using the Iris dataset in Python. We'll use the scikit-learn library for this task.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Select only two features for visualization purposes\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "C = 1.0  # Regularization parameter\n",
    "svm = SVC(kernel='linear', C=C)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the decision boundaries of the trained model\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Decision boundaries of SVM')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "This implementation uses the Iris dataset and splits it into a training set (80% of the data) and a testing set (20% of the data). It trains a linear SVM classifier using the training set, predicts the labels for the testing set, and computes the accuracy of the model. Finally, it plots the decision boundaries using the first two features (sepal length and sepal width).\n",
    "\n",
    "To try different values of the regularization parameter C, you can modify the C variable in the code and observe how it affects the performance of the model. Lower values of C provide a larger margin but may result in more misclassifications, while higher values of C make the model more sensitive to individual data points and may overfit the training data.\n",
    "\n",
    "You can run this code and experiment with different values of C to observe the performance and decision boundaries of the SVM classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6060d5-2fc3-4f35-b982-9deff7b2ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q5.\n",
    "\n",
    "Sure! Let's illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in Support Vector Machines (SVM) using simple examples and graphs.\n",
    "\n",
    "Hyperplane:\n",
    "A hyperplane is a decision boundary that separates the data points of different classes in an SVM. In a binary classification problem, a hyperplane is a linear subspace of one dimension less than the input space. For instance, in a 2D input space, the hyperplane is a line, and in a 3D input space, it is a plane. The hyperplane aims to maximize the margin between the classes.\n",
    "Example:\n",
    "Let's consider a 2D dataset with two classes, labeled as blue and red. The hyperplane (line in this case) is the decision boundary that separates the two classes.\n",
    "\n",
    "Marginal plane:\n",
    "The marginal plane is the boundary parallel to the hyperplane and located at an equal distance from it. It defines the region where the support vectors lie. The distance between the marginal plane and the hyperplane is known as the margin.\n",
    "Example:\n",
    "In the same 2D dataset, the marginal planes (dashed lines) are parallel to the hyperplane (solid line) and equidistant from it. The support vectors (marked with crosses) lie on the marginal planes.\n",
    "\n",
    "Soft Margin:\n",
    "In a soft margin SVM, a certain degree of misclassification or overlap between classes is allowed. The soft margin SVM introduces slack variables (ξ) that permit some data points to fall within the margin or even be misclassified. The regularization parameter (C) controls the trade-off between maximizing the margin and minimizing the classification errors. A smaller C value allows more misclassifications, resulting in a wider margin, while a larger C value penalizes misclassifications, resulting in a narrower margin.\n",
    "Example:\n",
    "Consider a dataset that is not linearly separable. In a soft margin SVM, the margin can be adjusted to allow some misclassifications or points within the margin. The dashed lines represent the margins, and the misclassified or data points within the margin are circled.\n",
    "\n",
    "\n",
    "Hard Margin:\n",
    "In a hard margin SVM, no misclassification is allowed, and the classes are expected to be linearly separable without any data points falling within the margin. This means that the margin must be wide enough to accommodate all the data points while still separating the classes perfectly. Hard margin SVMs are more sensitive to outliers and noise.\n",
    "Example:\n",
    "For a dataset that is linearly separable, a hard margin SVM aims to find a hyperplane that perfectly separates the classes without any misclassifications or data points within the margin. The solid line represents the hyperplane, and the dashed lines show the margins.\n",
    "\n",
    "\n",
    "These examples and graphs illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM, providing a visual understanding of how SVMs handle classification tasks and adjust the decision boundaries based on the margin constraints and misclassification allowances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7238e-2c42-4221-8926-8b427616f36a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
